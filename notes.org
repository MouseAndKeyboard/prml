#+TITLE: Notes

* Polynomial Curve Fitting
** Generate Training Data
#+begin_src python
import jax.numpy as np
import math
import pandas as pd
import matplotlib.pyplot as plt
from jax import grad, jit, vmap
from jax import random

N = 50

key = random.PRNGKey(1)
x = random.uniform(key, shape=(N,), minval=0.0, maxval=4 * math.pi)
y = np.sin(x)
n = random.normal(key, shape=(N,)) * 0.5
n = random.permutation(key, n)
noised = y + n

frame = pd.DataFrame(np.dstack((x, noised)).reshape((N,2)), columns=["x", "y"])
org_table = [list(frame)] + [None] + frame.values.tolist()
return org_table
#+end_src

#+NAME: noisy_sin_data
#+RESULTS:
|                  x |                     y |
|--------------------+-----------------------|
| 2.7939138412475586 |    1.3944461345672607 |
| 11.607545852661133 |   -0.8483397960662842 |
|  7.745828628540039 |    0.8045932054519653 |
|  4.984571933746338 |   -0.3919447660446167 |
|  9.731599807739258 |    0.3510522246360779 |
|  8.163013458251953 |    0.3959655165672302 |
| 11.471037864685059 |   -1.2925783395767212 |
| 12.270416259765625 |  -0.14364174008369446 |
| 3.7128987312316895 |   -0.4577164649963379 |
|  1.411027193069458 |    0.7185168266296387 |
| 10.975133895874023 |    -1.393272876739502 |
|  7.445126533508301 |    1.1096861362457275 |
|  2.063825845718384 |    0.9322301745414734 |
|    6.2230544090271 |   -0.3425447940826416 |
|  9.510844230651855 |  -0.27619698643684387 |
| 11.431102752685547 |   -1.3952492475509644 |
|  8.019596099853516 |    0.6041476726531982 |
| 11.427510261535645 |  -0.19295597076416016 |
| 11.363163948059082 |   -0.9734664559364319 |
| 0.5972563624382019 |    1.2317025661468506 |
|    5.6692214012146 | -0.006463229656219482 |
| 10.171634674072266 |   -0.5623911023139954 |
|  4.420809745788574 |  -0.14152449369430542 |
|  2.636859178543091 |   0.23099055886268616 |
|  6.458907127380371 |    0.5515536665916443 |
|  6.796914577484131 |   0.39604613184928894 |
|  6.902308464050293 |  -0.13867545127868652 |
| 1.6686160564422607 |    1.8063609600067139 |
|  5.332601070404053 |   -0.6174987554550171 |
| 11.603372573852539 |   0.17178046703338623 |
|   2.70995831489563 |    1.1324008703231812 |
| 12.105293273925781 |   -0.9701769351959229 |
|  9.137463569641113 |    0.4601757824420929 |
|  4.806541442871094 |   -1.0569589138031006 |
| 12.109949111938477 |   0.45679837465286255 |
| 0.9452168345451355 |    0.6796225309371948 |
| 12.345970153808594 |   0.21459874510765076 |
|  5.984344005584717 |   -1.2437686920166016 |
| 0.3619265854358673 |    1.2492976188659668 |
| 3.8543665409088135 |  -0.30583375692367554 |
|  3.594871997833252 |  -0.13582870364189148 |
| 1.8439618349075317 |    1.6421363353729248 |
|   8.20157241821289 |    1.3781003952026367 |
|  10.13949203491211 |   -0.6613987684249878 |
| 10.966803550720215 |  -0.33113616704940796 |
|  4.427088260650635 |   -0.9420477747917175 |
| 11.908252716064453 |   -1.4462831020355225 |
|  7.111725330352783 |   0.12971508502960205 |
|  5.879851818084717 |   -0.5419571399688721 |
|  11.92192268371582 |   -0.5388507843017578 |

** Plot Training Data
#+begin_src python :results file :var data=noisy_sin_data
import jax.numpy as np
import matplotlib.pyplot as plt

reshaped = np.array(data).T

x = reshaped[0]
y = reshaped[1]
plt.plot(x, y, 'o')
plt.savefig("/tmp/polycurve.png")
return "/tmp/polycurve.png"
#+end_src

#+RESULTS:
[[file:/tmp/polycurve.png]]

** Polynomial Fit
#+begin_src python :var data=noisy_sin_data
import jax.numpy as np
import pandas as pd
from jax import grad, jit, vmap
from jax import random
from jax.config import config
config.update("jax_debug_nans", True)

reshaped = np.array(data).T
x = reshaped[0]
y = reshaped[1]

key = random.PRNGKey(0)
M = 4 # order of the polynomial
coefficients = random.normal(key, shape=(M,))

def polynomial(x, weights):
    total = 0
    for w, j in enumerate(weights):
        total += w * x**j
    return total

def sum_of_squared_error(weights, xs, targets):
    E = 0
    for x, t in zip(xs, targets):
        E += (polynomial(x, weights) - t)**2
    return 0.5 * E

def rms_error(weights, xs, targets):
    return (2 * sum_of_squared_error(weights, xs, targets) / len(targets))**0.5

step_size = 0.05

derivative_rms_e = grad(rms_error)
for i in range(40):
    grad_step = -1 * step_size * derivative_rms_e(coefficients, x, y) # -1 because we want to descend the gradient hill
    coefficients += grad_step


frame = pd.DataFrame(coefficients, columns=["coefficients"])
org_table = [list(frame)] + [None] + frame.values.tolist()
return org_table
#+end_src

#+NAME: polynomial_coefficients
#+RESULTS:
|        coefficients |
|---------------------|
|  1.8160858154296875 |
| -0.7906920313835144 |
| -0.7429272532463074 |
|  -0.774492621421814 |
** Evaluate the polynomial

#+begin_src python :var coefs=polynomial_coefficients data=noisy_sin_data
import jax.numpy as np
import pandas as pd
from jax import grad, jit, vmap
from jax import random

def polynomial(x, weights):
    total = 0
    for w, j in enumerate(weights):
        total += w * x**j
    return total

def sum_of_squared_error(weights, xs, targets):
    E = 0
    for x, t in zip(xs, targets):
        E += (polynomial(x, weights) - t)**2
    return 0.5 * E

def rms_error(weights, xs, targets):
    return (2 * sum_of_squared_error(weights, xs, targets) / len(targets))**0.5


reshaped = np.array(data).T
x = reshaped[0]
y = reshaped[1]

key = random.PRNGKey(1)
coefficients = random.normal(key, shape=(4,))

return rms_error(coefs, x, y)

#+end_src

#+RESULTS:
| 2.80316046 |
